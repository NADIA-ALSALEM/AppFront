{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NADIA-ALSALEM/AppFront/blob/main/Music_Genre_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L4hQkk3TnQG"
      },
      "outputs": [],
      "source": [
        "pip install lazypredict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "na-MxgjCTr6o"
      },
      "outputs": [],
      "source": [
        "from lazypredict.Supervised import LazyClassifier, LazyRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the training and testing data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Fill missing values using forward fill\n",
        "train_df.fillna(method='ffill', inplace=True)\n",
        "test_df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "# Fill missing values in specific columns using mean\n",
        "if 'instrumentalness' in train_df.columns:\n",
        "    train_df['instrumentalness'].fillna(train_df['instrumentalness'].mean(), inplace=True)\n",
        "if 'Popularity' in test_df.columns:\n",
        "    test_df['Popularity'].fillna(test_df['Popularity'].mean(), inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "for column in ['Artist Name', 'Track Name', 'key']:\n",
        "    le = LabelEncoder()\n",
        "    all_data = pd.concat([train_df[column], test_df[column]])\n",
        "    le.fit(all_data)\n",
        "    train_df[column] = le.transform(train_df[column])\n",
        "    test_df[column] = le.transform(test_df[column])\n",
        "    label_encoders[column] = le\n",
        "\n",
        "# Separate features and target\n",
        "X = train_df.drop(['Class', 'Id'], axis=1)\n",
        "y = train_df['Class']\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert the scaled data back to a DataFrame\n",
        "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize LazyClassifier\n",
        "clf = LazyClassifier(random_state=42)\n",
        "\n",
        "# Fit and evaluate models\n",
        "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# View the results\n",
        "print(models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkrDwx9RXYLP"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "# from sklearn.ensemble import StackingClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from xgboost import XGBClassifier\n",
        "# from lightgbm import LGBMClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Load the training and testing data\n",
        "# train_df = pd.read_csv('train.csv')\n",
        "# test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# # Fill missing values using forward fill\n",
        "# train_df.fillna(method='ffill', inplace=True)\n",
        "# test_df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "# # Fill missing values in specific columns using mean\n",
        "# if 'instrumentalness' in train_df.columns:\n",
        "#     train_df['instrumentalness'].fillna(train_df['instrumentalness'].mean(), inplace=True)\n",
        "# if 'Popularity' in test_df.columns:\n",
        "#     test_df['Popularity'].fillna(train_df['Popularity'].mean(), inplace=True)\n",
        "\n",
        "# # Encode categorical variables\n",
        "# label_encoders = {}\n",
        "# for column in ['Artist Name', 'Track Name', 'key']:\n",
        "#     le = LabelEncoder()\n",
        "#     all_data = pd.concat([train_df[column], test_df[column]])\n",
        "#     le.fit(all_data)\n",
        "#     train_df[column] = le.transform(train_df[column])\n",
        "#     test_df[column] = le.transform(test_df[column])\n",
        "#     label_encoders[column] = le\n",
        "\n",
        "# # Separate features and target in training data\n",
        "# X_train = train_df.drop(['Class', 'Id'], axis=1)\n",
        "# y_train = train_df['Class']\n",
        "\n",
        "# # Prepare test data\n",
        "# X_test = test_df.drop(['Id'], axis=1)\n",
        "\n",
        "# # Feature scaling\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# # Convert the scaled data back to a DataFrame\n",
        "# X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "# X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# # Split the training data for validation\n",
        "# X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Initialize the base classifiers\n",
        "# clf1 = RandomForestClassifier(random_state=42)\n",
        "# clf2 = XGBClassifier(random_state=42)\n",
        "# clf3 = LGBMClassifier(random_state=42)\n",
        "\n",
        "# # Initialize the StackingClassifier\n",
        "# stacking_clf = StackingClassifier(\n",
        "#     estimators=[\n",
        "#         ('rf', clf1), ('xgb', clf2), ('lgbm', clf3)],\n",
        "#     final_estimator=LogisticRegression()\n",
        "# )\n",
        "\n",
        "# # Fit the StackingClassifier\n",
        "# stacking_clf.fit(X_train_split, y_train_split)\n",
        "\n",
        "# # Make predictions on the validation data\n",
        "# y_val_pred = stacking_clf.predict(X_val_split)\n",
        "\n",
        "# # Evaluate the model\n",
        "# accuracy = accuracy_score(y_val_split, y_val_pred)\n",
        "# print(f'Validation Accuracy: {accuracy}')\n",
        "# print('Classification Report:')\n",
        "# print(classification_report(y_val_split, y_val_pred))\n",
        "\n",
        "# # Make predictions on the test data\n",
        "# test_scaled = scaler.transform(test_df.drop(['Id'], axis=1))  # Scale the test data\n",
        "# test_pred = stacking_clf.predict(test_scaled)\n",
        "# print(test_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import  os\n",
        "# import sys\n",
        "\n",
        "# devnull = open(os.devnull, 'w')\n",
        "# old_stdout = sys.stdout\n",
        "# old_stderr = sys.stderr\n",
        "# sys.stdout = devnull\n",
        "# sys.stderr = devnull"
      ],
      "metadata": {
        "id": "dCmoERkMBqas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost\n"
      ],
      "metadata": {
        "id": "FSVUI3P9E0Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_jmKmvfnuUO"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from skopt import BayesSearchCV\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.impute import KNNImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdfcxJXGnuQ9"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "y_train = train_df.Class\n",
        "train_df = train_df.drop(columns=['Id', 'Class'])\n",
        "test_df = pd.read_csv('test.csv')\n",
        "test_df = test_df.drop(columns=['Id'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.read_csv(\"train.csv\").drop('Id', axis=1)\n",
        "y = X.Class\n",
        "X.drop('Class', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "k5oUX9oVQPqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCPSmq8NA0dg"
      },
      "outputs": [],
      "source": [
        "cat_atts = ['Artist Name', 'Track Name']\n",
        "num_atts = np.setdiff1d(X.columns, cat_atts)\n",
        "imputer = KNNImputer()\n",
        "imputer.fit(X[num_atts])\n",
        "\n",
        "X[num_atts] = imputer.transform(train_df[num_atts])\n",
        "X['mode'] = X['mode'].astype(int)\n",
        "X['key'] = X['key'].astype(int)\n",
        "X['time_signature'] = X['time_signature'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dist = y.value_counts(normalize=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "iVONjbQTRQke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YctBW52pnuGX"
      },
      "outputs": [],
      "source": [
        "# Define the base models for stacking\n",
        "base_models = [\n",
        "    ('lgbm1', LGBMClassifier(n_estimators=100, learning_rate=0.1)),\n",
        "    ('lgbm2', LGBMClassifier(n_estimators=200, learning_rate=0.05)),\n",
        "    ('lgbm3', LGBMClassifier(n_estimators=300, learning_rate=0.01))\n",
        "]\n",
        "\n",
        "# Define the stacking classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(), cv=StratifiedKFold(n_splits=5))\n",
        "\n",
        "# Train the stacking classifier\n",
        "stacking_clf.fit(X_train_split, y_train_split)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = stacking_clf.predict(X_val_split)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "accuracy = accuracy_score(y_val_split, y_test_pred)\n",
        "print(f'Stacking Classifier Validation Accuracy: {accuracy}')\n",
        "print('Stacking Classifier Classification Report:')\n",
        "print(classification_report(y_val_split, y_test_pred))\n"
      ],
      "metadata": {
        "id": "2uCWZmveF45l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the Id and the predictions into a DataFrame\n",
        "results_df = pd.DataFrame({'Id': test_df['Id'], 'Class': y_test_pred})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "results_df.to_csv('predictions.csv', index=False)\n",
        "print(\"Predictions have been saved to 'predictions.csv'\")\n"
      ],
      "metadata": {
        "id": "eiTnf1-EJjmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTS1JJpgA0di"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test data\n",
        "test_scaled = scaler.transform(test_df)  # Scale the test data\n",
        "test_pred = stacking_clf.predict(test_scaled)\n",
        "pd.Series(test_pred).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dulrC43PA0dj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVuBVxxAA0dj"
      },
      "outputs": [],
      "source": [
        "sybmit = pd.DataFrame({'Id': pd.read_csv('test (2).csv').Id, 'Class': test_pred})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URS9giJlA0dj"
      },
      "outputs": [],
      "source": [
        "sybmit.to_csv(\"./submit.csv\", index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EwgN4auA0dk"
      },
      "outputs": [],
      "source": [
        "pd.read_csv('./submit.csv').Class.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiBgQhgjA0dk"
      },
      "outputs": [],
      "source": [
        "# best\n",
        "10    1297\n",
        "9      456\n",
        "6      437\n",
        "8      324\n",
        "5      276\n",
        "2      222\n",
        "1      177\n",
        "0      143\n",
        "7      106\n",
        "4       83\n",
        "3       79"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2nNDKbqA0dk"
      },
      "outputs": [],
      "source": [
        "pd.read_csv('train (1).csv').Class.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MF7rITqA0dk"
      },
      "outputs": [],
      "source": [
        "- Clustering [Id, and probs]\n",
        "- just predict for those artists who are not in the train set\n",
        "- something better than logistic\n",
        "- tuning\n",
        "- prob for each clf\n",
        "- CatBoostClassifier"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}